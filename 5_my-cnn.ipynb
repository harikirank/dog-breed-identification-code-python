{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-30T05:52:23.523156Z",
     "iopub.status.busy": "2023-04-30T05:52:23.522552Z",
     "iopub.status.idle": "2023-04-30T05:52:35.660207Z",
     "shell.execute_reply": "2023-04-30T05:52:35.659042Z",
     "shell.execute_reply.started": "2023-04-30T05:52:23.523120Z"
    },
    "id": "-0OvcGz706f5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tq\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "from PIL import ImageFile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report, precision_recall_fscore_support, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import sys\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import pprint\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXpBfP6i1BaD",
    "outputId": "bf3b0410-93a2-41be-b6a7-1f373b9b3a1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T05:52:35.663185Z",
     "iopub.status.busy": "2023-04-30T05:52:35.662344Z",
     "iopub.status.idle": "2023-04-30T05:52:35.671899Z",
     "shell.execute_reply": "2023-04-30T05:52:35.670769Z",
     "shell.execute_reply.started": "2023-04-30T05:52:35.663140Z"
    },
    "id": "5zhQx0cV06f6"
   },
   "outputs": [],
   "source": [
    "def path_given_id(id, test=False):\n",
    "    \"\"\"\n",
    "    Returns the full path to the image given the id of the image.\n",
    "    Parameters:\n",
    "        - id: The id of the image.\n",
    "        - test: If True returns the relative path from the test folder. Otherwise, returns the relative path to the image from the training folder.\n",
    "    Returns:\n",
    "        - The full relative path to the image with the give in id.\n",
    "    \"\"\"\n",
    "    return IMAGES_PATH + ('train/' if not test else 'test/') + str(id) + '.jpg'\n",
    "\n",
    "def get_img_array(id, test=False):\n",
    "    \"\"\"\n",
    "    Loads the image from the given id, convert the image to a numpy array and return the numpy array.\n",
    "    Parameters:\n",
    "        - id: The id of the image.\n",
    "        - test: If True, loads the image from the test folder. If False,loads the image from the train folder.\n",
    "    Returns:\n",
    "        - The image with the give id as a numpy array.\n",
    "    \"\"\"\n",
    "    img = load_img(path_given_id(id, test), target_size=(224, 224))\n",
    "    return img_to_array(img)\n",
    "\n",
    "# preprocess_input(np.expand_dims(get_image_array(id, test), axis=0)) will convert the image into 1,224,224,3 to give to predict.\n",
    "def process_image(id, test=False):\n",
    "    return preprocess_input(np.expand_dims(get_img_array(id, test), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T05:52:35.674708Z",
     "iopub.status.busy": "2023-04-30T05:52:35.673527Z",
     "iopub.status.idle": "2023-04-30T05:52:36.238428Z",
     "shell.execute_reply": "2023-04-30T05:52:36.237370Z",
     "shell.execute_reply.started": "2023-04-30T05:52:35.674667Z"
    },
    "id": "K854hsMX06f6"
   },
   "outputs": [],
   "source": [
    "IMAGES_PATH = '/content/drive/MyDrive/dog_breed_identification_files/'\n",
    "\n",
    "labels = pd.read_csv(IMAGES_PATH +'labels.csv')\n",
    "labelnames = pd.read_csv(IMAGES_PATH  + 'sample_submission.csv').keys()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T05:52:37.724400Z",
     "iopub.status.busy": "2023-04-30T05:52:37.721812Z",
     "iopub.status.idle": "2023-04-30T05:52:37.817186Z",
     "shell.execute_reply": "2023-04-30T05:52:37.816103Z",
     "shell.execute_reply.started": "2023-04-30T05:52:37.724336Z"
    },
    "id": "jfvz_Mpe06f6"
   },
   "outputs": [],
   "source": [
    "codes = range(len(labelnames))\n",
    "breed_to_code = dict(zip(labelnames, codes))\n",
    "code_to_breed = dict(zip(codes, labelnames))\n",
    "\n",
    "labels['target'] =  [breed_to_code[x] for x in labels.breed]\n",
    "labels['rank'] = labels.groupby('breed').rank()['id']\n",
    "labels_pivot = labels.pivot('id', 'breed', 'target').reset_index().fillna(0)\n",
    "\n",
    "training_data = labels_pivot.sample(frac=0.85)\n",
    "validation_data = labels_pivot[~labels_pivot['id'].isin(training_data['id'])]\n",
    "testing_data = training_data.sample(frac=0.25)\n",
    "training_data = training_data[~training_data['id'].isin(testing_data['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T05:52:38.884320Z",
     "iopub.status.busy": "2023-04-30T05:52:38.882258Z",
     "iopub.status.idle": "2023-04-30T05:52:38.892922Z",
     "shell.execute_reply": "2023-04-30T05:52:38.891680Z",
     "shell.execute_reply.started": "2023-04-30T05:52:38.884273Z"
    },
    "id": "Fp3gRPC906f6"
   },
   "outputs": [],
   "source": [
    "img_transform = {\n",
    "    'valid':transforms.Compose([\n",
    "        transforms.Resize(size = 224, interpolation=InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(size = 224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'train':transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size = 224),\n",
    "        transforms.RandomRotation(degrees = 30),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])  \n",
    "    ]),\n",
    "    'test':transforms.Compose([\n",
    "        transforms.Resize(size = 224, interpolation=InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(size = 224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T05:52:39.616512Z",
     "iopub.status.busy": "2023-04-30T05:52:39.615543Z",
     "iopub.status.idle": "2023-04-30T05:52:39.625448Z",
     "shell.execute_reply": "2023-04-30T05:52:39.624261Z",
     "shell.execute_reply.started": "2023-04-30T05:52:39.616462Z"
    },
    "id": "-mNwv1IV06f7"
   },
   "outputs": [],
   "source": [
    "class DogDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Create a dataset for pytorch batch loading. This is to load few images into memory at a time instead of all the images at once.\n",
    "    Extends from torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, images_directory, labels, transform):\n",
    "        \"\"\"\n",
    "        Constructor initialization.\n",
    "        Params:\n",
    "            - images_directory: The directory where the images are stored.\n",
    "            - labels: The image labels\n",
    "            - transform: The transformations to perform on the data.\n",
    "        \"\"\"\n",
    "        self.images_directory = images_directory\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.labels is not None:\n",
    "            image_name = f'{self.labels[\"id\"].iloc[index]}.jpg'\n",
    "            full_image_name = self.images_directory + image_name\n",
    "            \n",
    "            final_image = Image.open(full_image_name)\n",
    "            label = self.labels.iloc[index, 1:].astype('float').to_numpy()\n",
    "            label = np.argmax(label)\n",
    "            \n",
    "            if self.transform:\n",
    "                final_image = self.transform(final_image)\n",
    "            \n",
    "            return [final_image, label]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T05:52:40.506463Z",
     "iopub.status.busy": "2023-04-30T05:52:40.505493Z",
     "iopub.status.idle": "2023-04-30T05:52:40.515401Z",
     "shell.execute_reply": "2023-04-30T05:52:40.514400Z",
     "shell.execute_reply.started": "2023-04-30T05:52:40.506414Z"
    },
    "id": "kK6JbNIM06f7"
   },
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "batch_size = 50\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "train_img = DogDataset(IMAGES_PATH + 'train/', training_data, transform = img_transform['train'])\n",
    "valid_img = DogDataset(IMAGES_PATH + 'train/', validation_data, transform = img_transform['valid'])\n",
    "test_img = DogDataset(IMAGES_PATH + 'train/', testing_data, transform = img_transform['test'])\n",
    "\n",
    "\n",
    "dataloaders={\n",
    "    'train':torch.utils.data.DataLoader(train_img, batch_size, num_workers = num_workers, shuffle=True),\n",
    "    'valid':torch.utils.data.DataLoader(valid_img, batch_size, num_workers = num_workers, shuffle=False),\n",
    "    'test':torch.utils.data.DataLoader(test_img, batch_size, num_workers = num_workers, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T06:21:23.743112Z",
     "iopub.status.busy": "2023-04-30T06:21:23.741939Z",
     "iopub.status.idle": "2023-04-30T06:21:23.755661Z",
     "shell.execute_reply": "2023-04-30T06:21:23.754392Z",
     "shell.execute_reply.started": "2023-04-30T06:21:23.743038Z"
    },
    "id": "cDCc75QR06f7"
   },
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, num_classes=120):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 128, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(28 * 28 * 64, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 128)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T06:21:24.520856Z",
     "iopub.status.busy": "2023-04-30T06:21:24.519887Z",
     "iopub.status.idle": "2023-04-30T06:21:25.077516Z",
     "shell.execute_reply": "2023-04-30T06:21:25.076402Z",
     "shell.execute_reply.started": "2023-04-30T06:21:24.520817Z"
    },
    "id": "8CYEVyJc06f7"
   },
   "outputs": [],
   "source": [
    "myModel = MyCNN()\n",
    "\n",
    "if use_cuda:\n",
    "    myModel = myModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-30T06:21:25.079795Z",
     "iopub.status.busy": "2023-04-30T06:21:25.079428Z",
     "iopub.status.idle": "2023-04-30T06:21:25.085331Z",
     "shell.execute_reply": "2023-04-30T06:21:25.084214Z",
     "shell.execute_reply.started": "2023-04-30T06:21:25.079766Z"
    },
    "id": "Wx_x4HwV06f8",
    "outputId": "8826a7b9-815a-49b9-8de1-9dac61efe2ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=50176, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=120, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T06:21:25.582321Z",
     "iopub.status.busy": "2023-04-30T06:21:25.581446Z",
     "iopub.status.idle": "2023-04-30T06:21:25.588846Z",
     "shell.execute_reply": "2023-04-30T06:21:25.587523Z",
     "shell.execute_reply.started": "2023-04-30T06:21:25.582277Z"
    },
    "id": "qdv9AwEo06f8"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# use Stochastic Gradient Descent to minimize the loss.\n",
    "optimizer = torch.optim.SGD(myModel.parameters(), lr=0.01, momentum=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T06:21:26.099345Z",
     "iopub.status.busy": "2023-04-30T06:21:26.098484Z",
     "iopub.status.idle": "2023-04-30T06:21:26.112220Z",
     "shell.execute_reply": "2023-04-30T06:21:26.110980Z",
     "shell.execute_reply.started": "2023-04-30T06:21:26.099298Z"
    },
    "id": "jOy7HlOI06f8"
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, img_transforms, model, optimizer, criterion, use_cuda):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        loss_during_train = 0.0\n",
    "        loss_during_validation = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for index_batch, (image, label) in enumerate(img_transforms['train']):\n",
    "            if use_cuda:\n",
    "                image, label = image.cuda(), label.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(image)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_during_train = loss_during_train + ((1 / (index_batch + 1)) * (loss.data - loss_during_train))\n",
    "            \n",
    "            if index_batch % 10 == 0:\n",
    "                print(f'Epoch: {epoch} \\tBatch: {index_batch + 1} \\tTraining Loss: {loss_during_train:.2f}')\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in img_transforms['valid']:\n",
    "                if use_cuda:\n",
    "                    images = images.cuda()\n",
    "                    labels = labels.cuda()\n",
    "\n",
    "                outputs = model(images)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        print(f'Accuracy of the network on the {total} validation images: {100 * correct / total} %') \n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        for index_batch, (image, label) in enumerate(img_transforms['valid']):\n",
    "            if use_cuda:\n",
    "                image, label = image.cuda(), label.cuda()\n",
    "\n",
    "            output = model(image)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            loss_during_validation = loss_during_validation + ((1 / (index_batch + 1)) * (loss.data - loss_during_validation))\n",
    "            \n",
    "        print(f'Epoch: {epoch} \\tTraining Loss: {loss_during_train:.2f} \\tValidation Loss: {loss_during_validation:.2f}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-04-30T06:21:41.737906Z",
     "iopub.status.busy": "2023-04-30T06:21:41.736829Z",
     "iopub.status.idle": "2023-04-30T06:21:44.474558Z",
     "shell.execute_reply": "2023-04-30T06:21:44.473046Z",
     "shell.execute_reply.started": "2023-04-30T06:21:41.737854Z"
    },
    "id": "ayshxMla06f9",
    "outputId": "882ac947-796a-47ca-ed50-d874a5a3bda8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 1 \tTraining Loss: 4.85\n",
      "Epoch: 1 \tBatch: 11 \tTraining Loss: 4.93\n",
      "Epoch: 1 \tBatch: 21 \tTraining Loss: 4.87\n",
      "Epoch: 1 \tBatch: 31 \tTraining Loss: 4.84\n",
      "Epoch: 1 \tBatch: 41 \tTraining Loss: 4.83\n",
      "Epoch: 1 \tBatch: 51 \tTraining Loss: 4.82\n",
      "Epoch: 1 \tBatch: 61 \tTraining Loss: 4.82\n",
      "Epoch: 1 \tBatch: 71 \tTraining Loss: 4.81\n",
      "Epoch: 1 \tBatch: 81 \tTraining Loss: 4.81\n",
      "Epoch: 1 \tBatch: 91 \tTraining Loss: 4.81\n",
      "Epoch: 1 \tBatch: 101 \tTraining Loss: 4.81\n",
      "Epoch: 1 \tBatch: 111 \tTraining Loss: 4.81\n",
      "Epoch: 1 \tBatch: 121 \tTraining Loss: 4.80\n",
      "Epoch: 1 \tBatch: 131 \tTraining Loss: 4.80\n",
      "Accuracy of the network on the 1533 validation images: 1.304631441617743 %\n",
      "Epoch: 1 \tTraining Loss: 4.80 \tValidation Loss: 4.79\n",
      "Epoch: 2 \tBatch: 1 \tTraining Loss: 4.80\n",
      "Epoch: 2 \tBatch: 11 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 21 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 31 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 41 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 51 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 61 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 71 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 81 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 91 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 101 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 111 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 121 \tTraining Loss: 4.79\n",
      "Epoch: 2 \tBatch: 131 \tTraining Loss: 4.79\n",
      "Accuracy of the network on the 1533 validation images: 0.45662100456621 %\n",
      "Epoch: 2 \tTraining Loss: 4.79 \tValidation Loss: 4.79\n",
      "Epoch: 3 \tBatch: 1 \tTraining Loss: 4.79\n",
      "Epoch: 3 \tBatch: 11 \tTraining Loss: 4.79\n",
      "Epoch: 3 \tBatch: 21 \tTraining Loss: 4.79\n",
      "Epoch: 3 \tBatch: 31 \tTraining Loss: 4.79\n",
      "Epoch: 3 \tBatch: 41 \tTraining Loss: 4.79\n",
      "Epoch: 3 \tBatch: 51 \tTraining Loss: 4.79\n",
      "Epoch: 3 \tBatch: 61 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 91 \tTraining Loss: 4.79\n",
      "Epoch: 3 \tBatch: 101 \tTraining Loss: 4.79\n",
      "Epoch: 3 \tBatch: 111 \tTraining Loss: 4.79\n",
      "Epoch: 3 \tBatch: 121 \tTraining Loss: 4.79\n",
      "Epoch: 3 \tBatch: 131 \tTraining Loss: 4.79\n",
      "Accuracy of the network on the 1533 validation images: 0.45662100456621 %\n",
      "Epoch: 3 \tTraining Loss: 4.79 \tValidation Loss: 4.79\n",
      "Epoch: 4 \tBatch: 1 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 11 \tTraining Loss: 4.79\n",
      "Epoch: 4 \tBatch: 21 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 31 \tTraining Loss: 4.79\n",
      "Epoch: 4 \tBatch: 41 \tTraining Loss: 4.79\n",
      "Epoch: 4 \tBatch: 51 \tTraining Loss: 4.79\n",
      "Epoch: 4 \tBatch: 61 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 91 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 101 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 121 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 131 \tTraining Loss: 4.78\n",
      "Accuracy of the network on the 1533 validation images: 1.1741682974559686 %\n",
      "Epoch: 4 \tTraining Loss: 4.78 \tValidation Loss: 4.79\n",
      "Epoch: 5 \tBatch: 1 \tTraining Loss: 4.79\n",
      "Epoch: 5 \tBatch: 11 \tTraining Loss: 4.79\n",
      "Epoch: 5 \tBatch: 21 \tTraining Loss: 4.79\n",
      "Epoch: 5 \tBatch: 31 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 41 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 51 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 61 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 91 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 101 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 121 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 131 \tTraining Loss: 4.78\n",
      "Accuracy of the network on the 1533 validation images: 1.1741682974559686 %\n",
      "Epoch: 5 \tTraining Loss: 4.78 \tValidation Loss: 4.79\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "output_model =  train(n_epochs, dataloaders, myModel, optimizer, loss_function, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_sBVXDA9DcZ",
    "outputId": "4137f7e8-c367-43c3-cee8-2b1ed2f6abad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 1 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 11 \tTraining Loss: 4.79\n",
      "Epoch: 1 \tBatch: 21 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 31 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 41 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 51 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 61 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 91 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 101 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 121 \tTraining Loss: 4.78\n",
      "Epoch: 1 \tBatch: 131 \tTraining Loss: 4.78\n",
      "Accuracy of the network on the 1533 validation images: 0.91324200913242 %\n",
      "Epoch: 1 \tTraining Loss: 4.78 \tValidation Loss: 4.79\n",
      "Epoch: 2 \tBatch: 1 \tTraining Loss: 4.75\n",
      "Epoch: 2 \tBatch: 11 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 21 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 31 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 41 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 51 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 61 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 91 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 101 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 121 \tTraining Loss: 4.78\n",
      "Epoch: 2 \tBatch: 131 \tTraining Loss: 4.78\n",
      "Accuracy of the network on the 1533 validation images: 0.91324200913242 %\n",
      "Epoch: 2 \tTraining Loss: 4.78 \tValidation Loss: 4.79\n",
      "Epoch: 3 \tBatch: 1 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 11 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 21 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 31 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 41 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 51 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 61 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 91 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 101 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 121 \tTraining Loss: 4.78\n",
      "Epoch: 3 \tBatch: 131 \tTraining Loss: 4.78\n",
      "Accuracy of the network on the 1533 validation images: 0.91324200913242 %\n",
      "Epoch: 3 \tTraining Loss: 4.78 \tValidation Loss: 4.79\n",
      "Epoch: 4 \tBatch: 1 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 11 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 21 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 31 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 41 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 51 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 61 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 91 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 101 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 121 \tTraining Loss: 4.78\n",
      "Epoch: 4 \tBatch: 131 \tTraining Loss: 4.78\n",
      "Accuracy of the network on the 1533 validation images: 1.1089367253750815 %\n",
      "Epoch: 4 \tTraining Loss: 4.78 \tValidation Loss: 4.79\n",
      "Epoch: 5 \tBatch: 1 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 11 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 21 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 31 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 41 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 51 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 61 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 91 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 101 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 121 \tTraining Loss: 4.78\n",
      "Epoch: 5 \tBatch: 131 \tTraining Loss: 4.78\n",
      "Accuracy of the network on the 1533 validation images: 1.1089367253750815 %\n",
      "Epoch: 5 \tTraining Loss: 4.78 \tValidation Loss: 4.79\n",
      "Epoch: 6 \tBatch: 1 \tTraining Loss: 4.77\n",
      "Epoch: 6 \tBatch: 11 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 21 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 31 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 41 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 51 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 61 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 91 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 101 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 121 \tTraining Loss: 4.78\n",
      "Epoch: 6 \tBatch: 131 \tTraining Loss: 4.78\n",
      "Accuracy of the network on the 1533 validation images: 1.1089367253750815 %\n",
      "Epoch: 6 \tTraining Loss: 4.78 \tValidation Loss: 4.79\n",
      "Epoch: 7 \tBatch: 1 \tTraining Loss: 4.76\n",
      "Epoch: 7 \tBatch: 11 \tTraining Loss: 4.78\n",
      "Epoch: 7 \tBatch: 21 \tTraining Loss: 4.77\n",
      "Epoch: 7 \tBatch: 31 \tTraining Loss: 4.77\n",
      "Epoch: 7 \tBatch: 41 \tTraining Loss: 4.77\n",
      "Epoch: 7 \tBatch: 51 \tTraining Loss: 4.77\n",
      "Epoch: 7 \tBatch: 61 \tTraining Loss: 4.77\n",
      "Epoch: 7 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 7 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 7 \tBatch: 91 \tTraining Loss: 4.78\n",
      "Epoch: 7 \tBatch: 101 \tTraining Loss: 4.78\n",
      "Epoch: 7 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 7 \tBatch: 121 \tTraining Loss: 4.78\n",
      "Epoch: 7 \tBatch: 131 \tTraining Loss: 4.78\n",
      "Accuracy of the network on the 1533 validation images: 1.1089367253750815 %\n",
      "Epoch: 7 \tTraining Loss: 4.78 \tValidation Loss: 4.79\n",
      "Epoch: 8 \tBatch: 1 \tTraining Loss: 4.79\n",
      "Epoch: 8 \tBatch: 11 \tTraining Loss: 4.78\n",
      "Epoch: 8 \tBatch: 21 \tTraining Loss: 4.78\n",
      "Epoch: 8 \tBatch: 31 \tTraining Loss: 4.78\n",
      "Epoch: 8 \tBatch: 41 \tTraining Loss: 4.77\n",
      "Epoch: 8 \tBatch: 51 \tTraining Loss: 4.77\n",
      "Epoch: 8 \tBatch: 61 \tTraining Loss: 4.78\n",
      "Epoch: 8 \tBatch: 71 \tTraining Loss: 4.78\n",
      "Epoch: 8 \tBatch: 81 \tTraining Loss: 4.78\n",
      "Epoch: 8 \tBatch: 91 \tTraining Loss: 4.77\n",
      "Epoch: 8 \tBatch: 101 \tTraining Loss: 4.77\n",
      "Epoch: 8 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 8 \tBatch: 121 \tTraining Loss: 4.78\n",
      "Epoch: 8 \tBatch: 131 \tTraining Loss: 4.78\n",
      "Accuracy of the network on the 1533 validation images: 1.1089367253750815 %\n",
      "Epoch: 8 \tTraining Loss: 4.78 \tValidation Loss: 4.79\n",
      "Epoch: 9 \tBatch: 1 \tTraining Loss: 4.77\n",
      "Epoch: 9 \tBatch: 11 \tTraining Loss: 4.78\n",
      "Epoch: 9 \tBatch: 21 \tTraining Loss: 4.78\n",
      "Epoch: 9 \tBatch: 31 \tTraining Loss: 4.78\n",
      "Epoch: 9 \tBatch: 41 \tTraining Loss: 4.78\n",
      "Epoch: 9 \tBatch: 51 \tTraining Loss: 4.78\n",
      "Epoch: 9 \tBatch: 61 \tTraining Loss: 4.77\n",
      "Epoch: 9 \tBatch: 71 \tTraining Loss: 4.77\n",
      "Epoch: 9 \tBatch: 81 \tTraining Loss: 4.77\n",
      "Epoch: 9 \tBatch: 91 \tTraining Loss: 4.78\n",
      "Epoch: 9 \tBatch: 101 \tTraining Loss: 4.77\n",
      "Epoch: 9 \tBatch: 111 \tTraining Loss: 4.78\n",
      "Epoch: 9 \tBatch: 121 \tTraining Loss: 4.77\n",
      "Epoch: 9 \tBatch: 131 \tTraining Loss: 4.77\n",
      "Accuracy of the network on the 1533 validation images: 1.1089367253750815 %\n",
      "Epoch: 9 \tTraining Loss: 4.77 \tValidation Loss: 4.79\n",
      "Epoch: 10 \tBatch: 1 \tTraining Loss: 4.80\n",
      "Epoch: 10 \tBatch: 11 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 21 \tTraining Loss: 4.78\n",
      "Epoch: 10 \tBatch: 31 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 41 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 51 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 61 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 71 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 81 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 91 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 101 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 111 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 121 \tTraining Loss: 4.77\n",
      "Epoch: 10 \tBatch: 131 \tTraining Loss: 4.77\n",
      "Accuracy of the network on the 1533 validation images: 1.1089367253750815 %\n",
      "Epoch: 10 \tTraining Loss: 4.77 \tValidation Loss: 4.79\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "output_model =  train(n_epochs, dataloaders, myModel, optimizer, loss_function, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuKD82VQ4cDL"
   },
   "source": [
    "# We are getting CUDA error because of lot of parameters. So, let's modify the FC layers sizes so we can build and run the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcgUjhl0Cmu9"
   },
   "source": [
    "# We can also clearly see that as we decrease the perceptrons we can't see much changes in the accuracy.\n",
    "# Also, these weights are randomly initialized, so they might take a lot of epochs to improve.\n",
    "# Finally, we can conclude that using pre trained models is the best possible solution and if we have the memory and computation power we can start our training with the pretrained weights and start training our model from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0n9P_6sNCj5v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
